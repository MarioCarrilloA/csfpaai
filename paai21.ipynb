{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAN--lUwyg-s"
   },
   "source": [
    "# PAAI21\n",
    "\n",
    "Image classification can be successfully solved by modern CNNs. However, there are still\n",
    "a plethora of questions regarding how those models manage to extract and model general\n",
    "features for large number of classes. A straightforward strategy is to focus on saliency i.e.,\n",
    "the area of the image that has a maximal response w.r.t. the predicted class. The aim of\n",
    "this project is to complement that idea by estimating the number of salient regions that\n",
    "common image classifiers react to and measure the consistency of those regions\n",
    "regarding the predicted class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMqc3_YRsmFQ"
   },
   "source": [
    "## Resnet18 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_kAyJN2sxVU"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage.transform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from torch import topk\n",
    "from torchvision import models, datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LNUnIXCyDwe",
    "outputId": "b4af7af2-3a03-4d91-8b90-c2d6ac4b3815"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Define paramters and other variables\n",
    "# -------------------------------------\n",
    "\n",
    "# Classes labels CIFAR-10\n",
    "classes = ('plane',\n",
    "           'auto',\n",
    "           'bird',\n",
    "           'cat',\n",
    "           'deer',\n",
    "           'dog',\n",
    "           'frog',\n",
    "           'horse',\n",
    "           'ship',\n",
    "           'truck')\n",
    "\n",
    "# CIFAR1-10 parameters\n",
    "input_size = 32\n",
    "num_classes = 10\n",
    "\n",
    "# Check for GPU/CPU to allocate tensor\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTMAQ2_0tBCI",
    "outputId": "e8bc7d52-2326-48e4-f03d-c023618e7396"
   },
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "normalize = transforms.Normalize(\n",
    "    (0.4914, 0.4822, 0.4465),\n",
    "    (0.2023, 0.1994, 0.2010)\n",
    ")\n",
    "    \n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# Download & transform CIFAR-10 datasets\n",
    "train_dataset = datasets.CIFAR10(\"./data\", train=True,\n",
    "                                 transform=train_transform, download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\"./data\", train=False,\n",
    "                                transform=test_transform, download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=5000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XktR_EJ_ERd"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=10)\n",
    "\n",
    "        self.resnet.conv1 = torch.nn.Conv2d(\n",
    "            3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.resnet.maxpool = torch.nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01PMfDH2H5we"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, epoch, verbose=False):\n",
    "  model.train()\n",
    "  total_loss = []\n",
    "  \n",
    "  for data, target in train_loader:\n",
    "    data = data.cuda()\n",
    "    target = target.cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(data)\n",
    "    loss = F.nll_loss(prediction, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss.append(loss.item())\n",
    "    \n",
    "    avg_loss = sum(total_loss) / len(total_loss)\n",
    "    if (verbose):\n",
    "        print(\"Training set: Epoch: {} Average Loss: {:.2f}\".format(epoch, avg_loss))\n",
    "\n",
    "  return avg_loss\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, verbose=False):\n",
    "  model.eval()\n",
    "  loss = 0\n",
    "  correct = 0\n",
    "\n",
    "  for data, target in test_loader:\n",
    "    with torch.no_grad():\n",
    "      data = data.cuda()\n",
    "      target = target.cuda()\n",
    "\n",
    "      prediction = model(data)\n",
    "      loss += F.nll_loss(prediction, target, reduction=\"sum\")\n",
    "\n",
    "      prediction = prediction.max(1)[1]\n",
    "      correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "      loss /= len(test_loader.dataset)\n",
    "      percentage_correct = 100.0 * correct / len(test_loader.dataset)\n",
    "\n",
    "      if (verbose):\n",
    "        print(\"Testing set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\".format(\n",
    "            loss, correct, len(test_loader.dataset), percentage_correct))\n",
    "      \n",
    "  return loss, percentage_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FoYO8sgNWgm",
    "outputId": "ea929122-0a62-4e66-d136-979ef756b3ad"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs=50\n",
    "lr=0.1\n",
    "    \n",
    "model = Model()\n",
    "model = model.cuda()\n",
    "\n",
    "milestones = [25, 40]\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer, milestones=milestones, gamma=0.1\n",
    ")\n",
    "\n",
    "print(\"Start train/test resnet18!\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    avg_loss = train_model(model, train_loader, optimizer, epoch)\n",
    "    loss, percentage_correct = test_model(model, test_loader)\n",
    "    print(\"Epoch: {} Training: Loss: {:.2f} - Testing: Loss: {:.2f} Accuracy {:.2f}%\".format(\n",
    "        epoch, avg_loss, loss, percentage_correct))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"PAAI21_CIFAR10_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48QW9YtteIR4"
   },
   "source": [
    "# Class activation maps\n",
    "\n",
    "## Terminology\n",
    "- **Receptive field:** The receptive field of a neuron is composed by all\n",
    "  pixels in **X** input that influence it.\n",
    "\n",
    "- **Convolutional units:** A convolutional layer contains units whose receptive\n",
    "  fields cover a patch of the previous layer[1].\n",
    "\n",
    "- **Softmax:** It is a function used to become an output score in a probablity\n",
    "  distribution.\n",
    "\n",
    "- **Global average pooling (GAP):** It is an operation that consists of take the\n",
    "  averag of each feature map and the resulting vector is used to feed the\n",
    "  softmax layer[2]. \n",
    "\n",
    "## Summary\n",
    "\n",
    "1. The convolutional units of several CNNs layers behave as object detectors\n",
    "  even object location is not given. So they have the ability to localize\n",
    "  objects.\n",
    "\n",
    "2. This ability is lost when fully-connected layers are used for classification.\n",
    "\n",
    "3. we can replace fully-connected layers by GAP.\n",
    "\n",
    "4. There is no parameter to optimize in the global average pooling,\n",
    "  thus overfitting is avoided at this layer. So GAP acts as a regularizer.\n",
    "\n",
    "5. We can modify GAP and use it in combination with a class called \n",
    "  **class activation mapping (CAM)** to retain this localization ability\n",
    "  until the final layer.\n",
    "\n",
    "6. Therefore a CNN trained on object categorization is successfully able to\n",
    "  localize the discriminative regions for action classification.\n",
    "\n",
    "7. A **class activation map (CAM)** for a particular category indicates the\n",
    "  discriminative image regions used by the CNN to identify that category.\n",
    "\n",
    "8. we can identify the importance of the image regions by projecting back\n",
    "  the weights of the output layer on to the convolutional feature maps, a\n",
    "  technique we call **class activation mapping**.\n",
    "\n",
    "9. Normally, We perform GAP on the convulitional feature maps and this\n",
    "  output feed a fully-connected layer that produces the final output, So the\n",
    "  weighted sum of GAP output is used to generate the final output\n",
    "  (e.g. category of something).\n",
    " \n",
    "10. We can identify the importance of the image regions by projecting back\n",
    "   the weights of the output layer on to the convolutional feature maps.\n",
    " \n",
    "## CAM description\n",
    "\n",
    "$M_c(x, y) = \\sum_k w_k^c f_k(x,y)$\n",
    "<br><br>\n",
    "Where:<br>\n",
    "$M_c$ is the class activation map for a class $c$<br>\n",
    "$f_k(x,y)$ is the activation of unit $k$ in the last convolutional layer at spatial location $(x,y)$<br>\n",
    "$F^k$ is the output of GAP on $f_k(x,y)$ , then $F^k=\\sum_{x,y} f_k(x,y)$<br>\n",
    "$w_k^c$ indicates the importance of $F^k$ for class $c$\n",
    "\n",
    "\n",
    "## References:\n",
    "[1] https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
    "\n",
    "[2] https://arxiv.org/pdf/1312.4400v3.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = Model()\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load(\"../input/paaimodel/PAAI21_CIFAR10_model.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3v4Mh0ttXCd"
   },
   "outputs": [],
   "source": [
    "class LayerFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m):\n",
    "      self.hook = m.register_forward_hook(self.hook_fn)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "      self.features = ((output.cpu()).data).numpy()\n",
    "\n",
    "    def remove(self): self.hook.remove()\n",
    "\n",
    "\n",
    "def compute_CAM(feature_conv, class_weights):\n",
    "    _, num_channels, h, w = feature_conv.shape\n",
    "    CAM = np.zeros((h, w))\n",
    "    i = 0\n",
    "    for act_map in feature_conv[0]:\n",
    "        CAM += act_map * class_weights[i]\n",
    "        i+=1\n",
    "\n",
    "    # Now we need to normalize our CAM in [0,1] range\n",
    "    CAM = CAM - np.min(CAM)\n",
    "    CAM = CAM / np.max(CAM)\n",
    "\n",
    "    return CAM\n",
    "\n",
    "\n",
    "def get_one_random_sample(test_dataset):\n",
    "  num_total_imgs = len(test_dataset.data)\n",
    "  random_index = random.randint(1, num_total_imgs)\n",
    "  img = test_dataset.data[random_index]\n",
    "  label = test_dataset.targets[random_index]\n",
    "\n",
    "  return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuxxJt6OunKU"
   },
   "source": [
    "# Compute CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1WZj9GGs4m9"
   },
   "outputs": [],
   "source": [
    "display_transform = transforms.Compose([\n",
    "   transforms.Resize((32,32))])\n",
    "\n",
    "image, label =  get_one_random_sample(test_dataset)\n",
    "tensor = test_transform(image)\n",
    "\n",
    "prediction_var = Variable((tensor.unsqueeze(0)).cuda(), requires_grad=True)\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "model._modules.keys()\n",
    "\n",
    "final_layer = model._modules.get(\"resnet\").layer4[-1]\n",
    "activated_features = LayerFeatures(final_layer)\n",
    "\n",
    "prediction = model(prediction_var)\n",
    "pred_probabilities = F.softmax(prediction, dim=1).data.squeeze()\n",
    "activated_features.remove()\n",
    "\n",
    "# Indentify the predicted class\n",
    "value, index = topk(pred_probabilities, 1)\n",
    "\n",
    "# Get information from identified class \n",
    "weight_softmax_params = list(model._modules.get('resnet').fc.parameters())\n",
    "weight_softmax = np.squeeze(weight_softmax_params[0].cpu().data.numpy())\n",
    "class_id = topk(pred_probabilities,1)[1].int()\n",
    "class_weights = weight_softmax[class_id]\n",
    "img_cam = compute_CAM(activated_features.features, class_weights)\n",
    "\n",
    "# As we can see, our CAM size does not match with the our\n",
    "# image. We need to resize our map and interpolate the values\n",
    "# according to our image\n",
    "resized_cam = skimage.transform.resize(img_cam, tensor.shape[1:3])\n",
    "\n",
    "# Visualize results\n",
    "print(\"IMAGE CLASS: \", classes[label])\n",
    "print(\"PREDICTION: \", classes[index.tolist()[0]])\n",
    "print(\"ACCURACY: {:.2f}%\".format(value.tolist()[0] * 100))\n",
    "print(\"Shape:\", tensor.shape[1:3])\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(16, 16)) \n",
    "ax[0].imshow(image)\n",
    "ax[1].imshow(img_cam, alpha=0.5, cmap='jet')\n",
    "ax[2].imshow(image)\n",
    "ax[2].imshow(resized_cam, alpha=0.5, cmap='jet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
